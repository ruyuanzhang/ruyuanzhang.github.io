以下的论文旨在提供给心理学/脑科学的学生，希望从事AI for Neuroscience方向的研究者参考。AI for Neuroscience近些年来论文不少，每个人眼中的经典也不同，下面这些论文也仅仅代表了一些我个人的意见，水平有限，难免有疏漏。



## DNN在脑科学中的应用经典论文

1. Yamins, D. L., Hong, H., Cadieu, C. F., Solomon, E. A., Seibert, D., & DiCarlo, J. J. (2014). Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proceedings of the national academy of sciences, 111(23), 8619-8624. ==(神经网络和人脑客体识别的开端)==
2. Güçlü, U., & van Gerven, M. A. (2015). Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream. *Journal of Neuroscience*, *35*(27), 10005-10014. ==(神经网络和人类视皮层对应关系)==
3. Khaligh-Razavi, S. M., & Kriegeskorte, N. (2014). Deep supervised, but not unsupervised, models may explain IT cortical representation. *PLoS computational biology*, *10*(11), e1003915. ==(神经网络和人类视皮层的对应关系)==
4. Kell AJE*, Yamins D*, Shook EN, Norman-Haignere S, and McDermott JH (2018). A Task-Optimized Neural Network Replicates Human Auditory Behavior, Predicts Brain Responses, and Reveals a Cortical Processing Hierarchy. *Neuron.* **98**(3), pp. 630-644. ==(神经网络和人类听觉皮层对应关系)==
5. Bashivan, P., Kar, K., & DiCarlo, J. J. (2019). Neural population control via deep image synthesis. Science, 364(6439), eaav9436.  ==(closed-loop control of brain signals最强的神经网络对应人脑视皮层的证据)==
6. Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T., & Wang, X. J. (2019). Task representations in neural networks trained to perform many cognitive tasks. *Nature neuroscience*, *22*(2), 297-306. ==(循环神经网络对认识任务建模的经典)==
7. Wenliang, L. K., & Seitz, A. R. (2018). Deep neural networks for modeling visual perceptual learning. *Journal of Neuroscience*, *38*(27), 6028-6044. ==(神经网络解释知觉学习)==
8. Orhan, A. E., & Ma, W. J. (2019). A diverse range of factors affect the nature of neural representations underlying short-term memory. Nature neuroscience, 22(2), 275-283. ==(神经网络解释工作记忆的机制)==
9. Banino, A., Barry, C., Uria, B., Blundell, C., Lillicrap, T., Mirowski, P., ... & Kumaran, D. (2018). Vector-based navigation using grid-like representations in artificial agents. Nature, 557(7705), 429-433. ==(神经网络解释网格细胞)==
10. Whittington, J. C., Muller, T. H., Mark, S., Chen, G., Barry, C., Burgess, N., & Behrens, T. E. (2020). The Tolman-Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. *Cell*, *183*(5), 1249-1263. ==(Tolman-Eichenbaum machine on 认知地图)==



## 机器学习经典论文

1. Serre, T., Wolf, L., Bileschi, S., Riesenhuber, M., & Poggio, T. (2007). Robust object recognition with cortex-like mechanisms. *IEEE* *transactions on pattern analysis and* *machine intelligence*, *29*(3), 411-426. ==(深度学习前的类脑计算模型)==
2. Rao, R. P., & Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nature neuroscience, 2(1), 79-87. ==(Predictive Coding预测编码的起源)==
3. Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. *Advances in neural information processing systems*, *25*, 1097-1105. ==(AlexNet)==
4. He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In *Proceedings of the* *IEEE* *conference on* *computer vision and pattern recognition* (pp. 770-778). ==(ResNet)==
5. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Hassabis, D. (2015). Human-level control through deep reinforcement learning. *Nature*, *518*(7540), 529-533. ==(DQN)==
6. Silver D, Huang A, Maddison C J, et al. Mastering the game of Go with deep neural networks and tree search[J]. Nature, 2016, 529(7587): 484-489. ==(AlphaGo)==
7. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). Generative adversarial networks. https://arxiv.org/pdf/1406.2661.pdf. ==(GAN)==
8. Diederik P Kingma, Max Welling. (2014). Auto-Encoding Variational Bayes. https://arxiv.org/pdf/1312.6114.pdf. ==(VAE)==
9. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In  *Advances in neural information processing systems* (pp. 5998-6008). ==(Transformer)==
10. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In *Proceedings of the* *IEEE**/CVF Conference on* *Computer Vision and Pattern Recognition* (pp. 10684-10695). [Link](https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf) ==(stable diffusion model)==